version: '3'
# name: openmldb_sbin # 3.8 supports

services:
  deploy-node:
    container_name: deploy-node
    image: openmldb-deploy-node
    build:
      context: ./
      dockerfile: deploy-node/Dockerfile
    restart: always
    tty: true
    hostname: deploy-node
    volumes:
      # for deploy, default config and you can save the changes in deploy-node container
      - type: bind
        source: ./deploy-node/hosts
        target: /work/openmldb/conf/hosts
      - type: bind
        source: ./deploy-node/openmldb-env.sh
        target: /work/openmldb/conf/openmldb-env.sh
      - type: bind
        source: ./deploy-node/taskmanager.properties.template
        target: /work/openmldb/conf/taskmanager.properties.template
      # info log for spark-shell/sql
      - type: bind
        source: ./deploy-node/log4j.properties
        target: /work/openmldb/spark/conf/log4j.properties
      - type: bind
        source: ./hive/conf/hive-site.xml
        target: /work/openmldb/spark/conf/hive-site.xml
      # hive with postgre, so spark need postgre jar to connect
      - type: bind
        source: ./hive/postgresql-42.5.1.jar
        target: /work/openmldb/spark/jars/postgresql-42.5.1.jar

      # if you want to use hive cli in deploy-node. Normally, you can use beeline in any container.
      # - type: bind
      #   source: ./hive/conf/hive-site.xml
      #   target: /opt/hive/conf/hive-site.xml
      # if you want to use hadoop more simply, otherwise, run `hadoop fs -fs hdfs://namenode:9000 ...`
      # - type: bind
      #   source: ./hive/conf/core-site.xml
      #   target: /opt/hadoop/etc/hadoop/core-site.xml
      # - type: bind
      #   source: ./hive/postgresql-42.5.1.jar
      #   target: /opt/hive/lib/postgresql-42.5.1.jar

      # test new sbin if needed
      # - type: bind
      #   source: ../OpenMLDB-main/release/sbin/
      #   target: /work/openmldb/sbin/
      # test entire release
      - type: bind
        source: ../release-test/openmldb/
        target: /work/openmldb/
      # test resources mount, don't comment it
      - type: bind
        source: ./test/
        target: /work/test/
    command:
    - /bin/bash
    - -c
    - |
      echo "source /work/test/funcs.sh" >> ~/.bashrc
      /work/openmldb/sbin/deploy-all.sh && /work/openmldb/sbin/start-all.sh
      tail -f /dev/null
    # no extra host option or ip option, deploy-all ok. Each container for a service joins the default network and is both reachable 
    # by other containers on that network, and discoverable by the service's name.
    # `st` to check
    depends_on:
      - zk
      - ns
      - ts
      - tm
      - api

  zk:
    image: openmldb-runner
    build:
      context: ./
      dockerfile: runner/Dockerfile
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  ns:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  ts:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  tm:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2
    # extra_hosts: # for host hive, but it'll fail if hive use hdfs://localhost:xxx (hdfs can't be accessed by container), so create hive&hdfs in compose
    #   - "host.docker.internal:host-gateway"

  api:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

# HADOOP profile, ref https://github.com/big-data-europe/docker-hadoop/blob/master/hadoop.env
  # hdfs://namenode:9000
  namenode:
    profiles: [hadoop]
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9871:9870 # change localhost bind port, no effect if you are using docker-machine
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    profiles: [hadoop]
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
# HIVE profile, ref https://github.com/apache/hive/blob/master/packaging/src/docker/docker-compose.yml 
  postgres:
    profiles: [hive]
    # The postgres:16 image uses Debian as its base, and has some issues
    image: postgres:16-alpine
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hive'
      POSTGRES_PASSWORD: 'password'
    ports:
      - '5432:5432'
    volumes:
      - hive-db:/var/lib/postgresql
  # access cmd is `psql metastore_db hive`, don't forget the database arg, otherwrise, `FATAL:  database "hive" does not exist`
  metastore:
    profiles: [hive]
    image: apache/hive:${HIVE_VERSION}
    depends_on:
      - postgres
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore'
      SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
                     -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
                     -Djavax.jdo.option.ConnectionUserName=hive
                     -Djavax.jdo.option.ConnectionPassword=password'
      # HIVE_CUSTOM_CONF_DIR: /hive_custom_conf # can't replace hive-site.xml
    ports:
        - '9083:9083'
    volumes:
        - type: bind
          source: hive/postgresql-42.5.1.jar
          target: /opt/hive/lib/postgres.jar
        - type: bind
          source: hive/conf/hive-site.xml
          target: /opt/hive/conf/hive-site.xml
        # maybe needless(we always access hive by hs2)
        - type: bind
          source: hive/conf/core-site.xml
          target: /opt/hadoop/etc/hadoop/core-site.xml
  # hiveserver2 should have all configs 
  hiveserver2:
    profiles: [hive]
    image: apache/hive:${HIVE_VERSION}
    depends_on:
      - metastore
    restart: unless-stopped
    container_name: hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      SERVICE_OPTS: '-Xmx1G -Dhive.metastore.uris=thrift://metastore:9083'
      IS_RESUME: 'true'
      SERVICE_NAME: 'hiveserver2'
    ports:
      - '10000:10000'
      - '10002:10002'
    volumes:
      - type: bind
        source: hive/postgresql-42.5.1.jar
        target: /opt/hive/lib/postgres.jar
      - type: bind
        source: hive/conf/hive-site.xml
        target: /opt/hive/conf/hive-site.xml
      # for create db(hdfs dir) permission issue `/tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x`
      - type: bind
        source: hive/conf/core-site.xml
        target: /opt/hadoop/etc/hadoop/core-site.xml
# ICEBERG REST profile
  # https://hub.docker.com/r/tabulario/iceberg-rest and pr 43, can't use origin image
  # use config to connect rest service
  # spark.sql.catalog.rest                    org.apache.iceberg.spark.SparkCatalog
  # spark.sql.catalog.rest.catalog-impl       org.apache.iceberg.rest.RESTCatalog
  # spark.sql.catalog.rest.uri                http://0.0.0.0:8181/
  iceberg-rest:
    profiles: [rest]
    image: iceberg-rest-hive-patch
    container_name: iceberg-rest
    build:
      context: ./iceberg-rest-image
    restart: always
    environment: # __ means -
      - CATALOG_CATALOG__IMPL=org.apache.iceberg.hive.HiveCatalog
      - CATALOG_URI=thrift://metastore:9083
    depends_on:
      - metastore
volumes:
  hadoop_namenode:
  hadoop_datanode:
  hive-db:

networks:
  default:
    name: ${NETWORK_NAME}
    driver: bridge
