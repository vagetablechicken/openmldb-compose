version: '3'
# name: openmldb_sbin # 3.8 supports

services:
  deploy-node:
    container_name: deploy-node
    image: openmldb-deploy-node
    build:
      context: ./
      dockerfile: deploy-node/Dockerfile
    restart: always
    tty: true
    hostname: deploy-node
    volumes:
      # for deploy, default config and you can save the changes in deploy-node container
      - type: bind
        source: ./deploy-node/hosts
        target: /work/openmldb/conf/hosts
      - type: bind
        source: ./deploy-node/openmldb-env.sh
        target: /work/openmldb/conf/openmldb-env.sh
      - type: bind
        source: ./deploy-node/taskmanager.properties.template
        target: /work/openmldb/conf/taskmanager.properties.template
      # info log for spark-shell/sql
      - type: bind
        source: ./deploy-node/log4j.properties
        target: /work/openmldb/spark/conf/log4j.properties
      - type: bind
        source: ./hive/conf/hive-site.xml
        target: /work/openmldb/spark/conf/hive-site.xml
      # hive with postgre, so spark need postgre jar to connect
      - type: bind
        source: ./hive/postgresql-42.5.1.jar
        target: /work/openmldb/spark/jars/postgresql-42.5.1.jar

      # if you want to use hive cli in deploy-node. Normally, you can use beeline in any container.
      # - type: bind
      #   source: ./hive/conf/hive-site.xml
      #   target: /opt/hive/conf/hive-site.xml
      # if you want to use hadoop more simply, otherwise, run `hadoop fs -fs hdfs://namenode:9000 ...`
      # - type: bind
      #   source: ./hive/conf/core-site.xml
      #   target: /opt/hadoop/etc/hadoop/core-site.xml
      # - type: bind
      #   source: ./hive/postgresql-42.5.1.jar
      #   target: /opt/hive/lib/postgresql-42.5.1.jar

      # test new sbin if needed
      # - type: bind
      #   source: ../OpenMLDB-main/release/sbin/
      #   target: /work/openmldb/sbin/
      # test entire release
      - type: bind
        source: ../release-test/openmldb/
        target: /work/openmldb/
      # test resources mount, don't comment it
      - type: bind
        source: ./test/
        target: /work/test/
    command:
    - /bin/bash
    - -c
    - |
      echo "source /work/test/funcs.sh" >> ~/.bashrc
      /work/openmldb/sbin/deploy-all.sh && /work/openmldb/sbin/start-all.sh
      tail -f /dev/null
    # no extra host option or ip option, deploy-all ok. Each container for a service joins the default network and is both reachable 
    # by other containers on that network, and discoverable by the service's name.
    # `st` to check
    depends_on:
      - zk
      - ns
      - ts
      - tm
      - api

  zk:
    image: openmldb-runner
    build:
      context: ./
      dockerfile: runner/Dockerfile
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  ns:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  ts:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  tm:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2
    # extra_hosts: # for host hive, but it'll fail if hive use hdfs://localhost:xxx (hdfs can't be accessed by container), so create hive&hdfs in compose
    #   - "host.docker.internal:host-gateway"

  api:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  # https://hub.docker.com/r/tabulario/iceberg-rest
  # use config to connect rest service
  # spark.sql.catalog.rest                    org.apache.iceberg.spark.SparkCatalog
  # spark.sql.catalog.rest.catalog-impl       org.apache.iceberg.rest.RESTCatalog
  # spark.sql.catalog.rest.uri                http://0.0.0.0:8181/
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    environment: # __ means -
      - CATALOG_CATALOG__IMPL=org.apache.iceberg.hive.HiveCatalog
      - CATALOG_URI=thrift://metastore:9083
    depends_on:
      - metastore

networks:
  default:
    name: ${NETWORK_NAME}
    driver: bridge
