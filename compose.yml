version: '3'
# name: openmldb_sbin # 3.8 supports

services:
  deploy-node:
    container_name: deploy-node
    image: openmldb-deploy-node
    build:
      context: ./
      args:
        DEPLOY_NODE_IMAGE: ${DEPLOY_NODE_IMAGE}
        HIVE_VERSION: ${HIVE_VERSION}
      dockerfile: deploy-node/Dockerfile
    restart: always
    tty: true
    hostname: deploy-node
    volumes:
      # for deploy, default config and you can save the changes in deploy-node container
      - type: bind
        source: ./deploy-node/hosts
        target: /work/openmldb/conf/hosts
      - type: bind
        source: ./deploy-node/openmldb-env.sh
        target: /work/openmldb/conf/openmldb-env.sh
      - type: bind
        source: ./deploy-node/taskmanager.properties.template
        target: /work/openmldb/conf/taskmanager.properties.template
      # info log for spark-shell/sql
      - type: bind
        source: ./deploy-node/log4j.properties
        target: /work/openmldb/spark/conf/log4j.properties
      - type: bind
        source: ./hive/conf/hive-site.xml
        target: /work/openmldb/spark/conf/hive-site.xml
      # hive with postgre, so spark need postgre jar to connect
      - type: bind
        source: ./hive/postgresql-42.5.1.jar
        target: /work/openmldb/spark/jars/postgresql-42.5.1.jar
      # test resources mount, don't comment it
      - type: bind
        source: ./test/
        target: /work/test/

      # if you want to use hive cli in deploy-node. Normally, you can use beeline in any container.
      # - type: bind
      #   source: ./hive/conf/hive-site.xml
      #   target: /opt/hive/conf/hive-site.xml
      # if you want to use hadoop more simply, otherwise, run `hadoop fs -fs hdfs://namenode:9000 ...`
      # - type: bind
      #   source: ./hive/conf/core-site.xml
      #   target: /opt/hadoop/etc/hadoop/core-site.xml
      # - type: bind
      #   source: ./hive/postgresql-42.5.1.jar
      #   target: /opt/hive/lib/postgresql-42.5.1.jar

      - ${RELEASE_MOUNT:-/dev/null:/dummy}

      - type: bind
        source: ${EXTERNAL_MOUNT:-/dev/null}
        target: /work/ext/
    ports:
      - "4040-4049"
    command:
    - /bin/bash
    - -c
    - |
      echo "source /work/test/funcs.sh" >> ~/.bashrc
      tee -a ~/.bashrc << END
      export LC_ALL=C.UTF-8
      export LANG=C.UTF-8
      export LANGUAGE=C.UTF-8
      END
      /work/openmldb/sbin/deploy-all.sh && /work/openmldb/sbin/start-all.sh
      echo "SET GLOBAL deploy_stats = 'on';" | /work/openmldb/sbin/openmldb-cli.sh
      tail -f /dev/null
    # no extra host option or ip option, deploy-all ok. Each container for a service joins the default network and is both reachable 
    # by other containers on that network, and discoverable by the service's name.
    # `st` to check
    depends_on:
      - zk
      - ns
      - ts
      - tm
      - api

  zk:
    image: openmldb-runner
    build:
      context: ./
      dockerfile: runner/Dockerfile
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  ns:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

  ts:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  tm:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2
    # extra_hosts: # for host hive, but it'll fail if hive use hdfs://localhost:xxx (hdfs can't be accessed by container), so create hive&hdfs in compose
    #   - "host.docker.internal:host-gateway"

  api:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 1

# HADOOP profile, ref https://github.com/big-data-europe/docker-hadoop/blob/master/hadoop.env
  # hdfs://namenode:9000
  namenode:
    profiles: [hadoop, all]
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9871:9870 # change localhost bind port, no effect if you are using docker-machine
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    profiles: [hadoop, all]
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
# HIVE profile, ref https://github.com/apache/hive/blob/master/packaging/src/docker/docker-compose.yml 
  postgres:
    profiles: [hive, all]
    # The postgres:16 image uses Debian as its base, and has some issues
    image: postgres:16-alpine
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hive'
      POSTGRES_PASSWORD: 'password'
    ports:
      - '5432:5432'
    volumes:
      - hive-db:/var/lib/postgresql
  # access cmd is `psql metastore_db hive`, don't forget the database arg, otherwrise, `FATAL:  database "hive" does not exist`
  metastore:
    profiles: [hive, all]
    image: apache/hive:${HIVE_VERSION}
    depends_on:
      - postgres
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore'
      SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
                     -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
                     -Djavax.jdo.option.ConnectionUserName=hive
                     -Djavax.jdo.option.ConnectionPassword=password'
      # HIVE_CUSTOM_CONF_DIR: /hive_custom_conf # can't replace hive-site.xml
    ports:
        - '9083:9083'
    volumes:
        - type: bind
          source: hive/postgresql-42.5.1.jar
          target: /opt/hive/lib/postgres.jar
        - type: bind
          source: hive/conf/hive-site.xml
          target: /opt/hive/conf/hive-site.xml
        # maybe needless(we always access hive by hs2)
        - type: bind
          source: hive/conf/core-site.xml
          target: /opt/hadoop/etc/hadoop/core-site.xml
  # hiveserver2 should have all configs 
  hiveserver2:
    profiles: [hive, all]
    image: apache/hive:${HIVE_VERSION}
    depends_on:
      - metastore
    restart: unless-stopped
    container_name: hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      SERVICE_OPTS: '-Xmx1G -Dhive.metastore.uris=thrift://metastore:9083'
      IS_RESUME: 'true'
      SERVICE_NAME: 'hiveserver2'
    ports:
      - '10000:10000'
      - '10002:10002'
    volumes:
      - type: bind
        source: hive/postgresql-42.5.1.jar
        target: /opt/hive/lib/postgres.jar
      - type: bind
        source: hive/conf/hive-site.xml
        target: /opt/hive/conf/hive-site.xml
      # for create db(hdfs dir) permission issue `/tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x`
      - type: bind
        source: hive/conf/core-site.xml
        target: /opt/hadoop/etc/hadoop/core-site.xml
# ICEBERG REST profile
  # https://hub.docker.com/r/tabulario/iceberg-rest and pr 43, can't use origin image
  # use config to connect rest service
  # spark.sql.catalog.rest                    org.apache.iceberg.spark.SparkCatalog
  # spark.sql.catalog.rest.catalog-impl       org.apache.iceberg.rest.RESTCatalog
  # spark.sql.catalog.rest.uri                http://0.0.0.0:8181/
  iceberg-rest:
    profiles: [rest, all]
    image: iceberg-rest-hive-patch
    container_name: iceberg-rest
    build:
      context: ./iceberg-rest-image
    restart: always
    environment: # __ means -
      - CATALOG_CATALOG__IMPL=org.apache.iceberg.hive.HiveCatalog
      - CATALOG_URI=thrift://metastore:9083
    depends_on:
      - metastore
  # 9092
  # kafka cluster ref https://github.com/bitnami/containers/blob/7db812f196808feb47cba5f1f17377c6629c4281/bitnami/kafka/docker-compose-cluster.yml
  kafka-zookeeper:
    profiles: [kafka, all]
    image: docker.io/bitnami/zookeeper:3.9
    container_name: kafka-zookeeper
    # ports:
    #   - "2181:2181"
    volumes:
      - "kafka_zookeeper_data:/bitnami"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    # For error `Failed to start thread "GC Thread#0" - pthread_create failed (EPERM) for attributes`
    security_opt:
      - seccomp:unconfined
  kafka:
    profiles: [kafka, all]
    image: bitnami/kafka:3.5.1-debian-11-r75
    container_name: kafka
    restart: always
    # ports:
    #   - '9092:9092'
    # kraft mode has a lot of bugs
    # config is /opt/bitnami/kafka/config/server.properties
    environment:
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # don't zookeeper in openmldb cluster, it's not good for cleanup
      - KAFKA_CFG_ZOOKEEPER_CONNECT=kafka-zookeeper:2181
    depends_on:
      - kafka-zookeeper
    # if you want to persist data
    # volumes:
    #   # As this is a non-root container, the mounted files and directories must have the proper permissions for the UID 1001
    #   - './kafka_data:/bitnami/kafka'
  kafka-connect:
    profiles: [kafka, all]
    image: kafka-openmldb-plugin
    build:
      context: kafka
      args:
        DOWNLOAD_PLUGIN: true
    container_name: kafka-connect
    restart: always
    # ports:
    #   - '8079:8083'
    volumes:
      # debug level will print too much logs, qps will be slow
      #- './kafka/connect-log4j.properties:/opt/bitnami/kafka/config/connect-log4j.properties'
      - './kafka/connect-distributed.properties:/opt/bitnami/kafka/config/connect-distributed.properties'
      # you can bind your connector dir for test(disable DOWNLOAD_PLUGIN, skip download in image), otherwise use the official connector
      - './kafka/kafka-connect-jdbc:/opt/connectors/kafka-connect-jdbc'
    depends_on:
      - kafka
    command:     
      - /bin/bash
      - -c
      - |
        /opt/bitnami/kafka/bin/connect-distributed.sh /opt/bitnami/kafka/config/connect-distributed.properties

  prometheus:
    profiles: [monitor, all]
    image: prom/prometheus
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./monitor/prometheus.yml:/etc/prometheus/prometheus.yml
      - prome_storage:/prometheus
    ports:
      - 9093:9090 # visit your machine :9093
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.enable-lifecycle'
  # TODO store cfg in volume
  grafana:
    profiles: [monitor, all]
    image: grafana/grafana
    container_name: grafana
    restart: unless-stopped
    ports:
      - 3003:3000 # visit 3003
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_ALLOW_ORG_CREATE=false
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      - GF_AUTH_ANONYMOUS_ORG_NAME=Main Org.
      - GF_AUTH_ANONYMOUS_USER=true
      - GF_AUTH_ANONYMOUS_ORG_ID=1
    volumes:
      - grafana_storage:/var/lib/grafana
    # don't depends on prometheus to avoid create datasource and dashboard again
    command:
      - /bin/bash
      - -c
      - |
        # TODO: https://grafana.com/docs/grafana/latest/developers/http_api/create-api-tokens-for-org/
  openmldb-exporter:
    profiles: [monitor, all]
    image: ghcr.io/4paradigm/openmldb-exporter
    container_name: openmldb-exporter
    entrypoint: /bin/bash -c
    command:
      - |
        pip install openmldb==0.8.5 -i https://pypi.tuna.tsinghua.edu.cn/simple
        /usr/local/bin/openmldb-exporter --log.level=INFO --config.zk_root=openmldb-compose-zk-1:2181 --config.zk_path=/openmldb
    # to debug
    # volumes:
    #   - /home/huangwei/openmldb-exporter/openmldb_exporter:/usr/local/lib/python3.11/site-packages/openmldb_exporter
volumes:
  hadoop_namenode:
  hadoop_datanode:
  hive-db:
  kafka_zookeeper_data:
  prome_storage:
  grafana_storage:

networks:
  default:
    name: ${NETWORK_NAME}
    driver: ${NETWORK_MODE}
