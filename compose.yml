version: '3'
# test network
# name: openmldb_sbin # 3.8 supports
# Creating network "docker-cluster_default" with the default driver

services:
  deploy-node:
    container_name: deploy-node
    image: openmldb-deploy-node
    build:
      context: deploy-node
    restart: always
    tty: true
    hostname: deploy-node
    volumes:
      # for deploy, default config and you can save the changes in container
      - type: bind
        source: ./deploy-node/hosts
        target: /work/openmldb/conf/hosts
      - type: bind
        source: ./deploy-node/openmldb-env.sh
        target: /work/openmldb/conf/openmldb-env.sh
      - type: bind
        source: ./deploy-node/taskmanager.properties.template
        target: /work/openmldb/conf/taskmanager.properties.template
      # test new sbin
      # - type: bind
      #   source: ../OpenMLDB/release/sbin/
      #   target: /work/openmldb/sbin/
      # test resources mount
      - type: bind
        source: ./test/
        target: /work/test/
    command:
    - /bin/bash
    - -c
    - |
      /work/openmldb/sbin/deploy-all.sh && /work/openmldb/sbin/start-all.sh
      tail -f /dev/null
    # no extra host and ip, deploy-all ok. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by the service's name.
    # /work/openmldb/sbin/openmldb-cli.sh to check
    depends_on:
      - zk
      - ns
      - ts
      - tm
      - api

  zk:
    image: openmldb-runner
    build:
      context: runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 3
    # expose?
  ns:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  ts:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  tm:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  api:
    image: openmldb-runner
    restart: always
    tty: true
    deploy:
      mode: replicated
      replicas: 2

  # Hadoop cluster https://github.com/big-data-europe/docker-hadoop/blob/master/hadoop.env
  # address: hdfs://namenode:19000
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9871:9870 # 9870 confict, host use 9871, don't change port in container network
      - 19000:19000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=hdfs-test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
  
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:19000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:19000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
  
  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:19000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
  
volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
